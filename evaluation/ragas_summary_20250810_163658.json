{
  "evaluation_date": "2025-08-10T16:36:58.583588",
  "model_config": {
    "llm": "deepseek-r1-distill-llama-70b",
    "embedding": "BAAI/bge-base-en-v1.5",
    "reranking": "rerank-english-v3.0",
    "evaluation_llm": "deepseek-r1-distill-llama-70b (Groq)",
    "evaluation_embeddings": "BAAI/bge-base-en-v1.5 (Working HF)"
  },
  "metrics": {
    "faithfulness": {
      "mean": null,
      "std": null,
      "min": null,
      "max": null,
      "count": 0,
      "total": 2
    },
    "answer_relevancy": {
      "mean": 0.9999999999999991,
      "std": 0.0,
      "min": 0.9999999999999991,
      "max": 0.9999999999999991,
      "count": 1,
      "total": 2
    },
    "context_precision": {
      "mean": null,
      "std": null,
      "min": null,
      "max": null,
      "count": 0,
      "total": 2
    },
    "context_recall": {
      "mean": 0.7037037037037037,
      "std": 0.4190262407031393,
      "min": 0.4074074074074074,
      "max": 1.0,
      "count": 2,
      "total": 2
    },
    "semantic_similarity": {
      "mean": 0.9820762146534334,
      "std": 0.0074718614192754345,
      "min": 0.9767928107757776,
      "max": 0.9873596185310892,
      "count": 2,
      "total": 2
    },
    "answer_correctness": {
      "mean": null,
      "std": null,
      "min": null,
      "max": null,
      "count": 0,
      "total": 2
    }
  },
  "notes": {
    "rate_limiting": "Some evaluations failed due to Groq rate limits",
    "successful_metrics": [
      "faithfulness",
      "answer_relevancy",
      "context_precision",
      "context_recall",
      "semantic_similarity",
      "answer_correctness"
    ],
    "total_questions": 2
  }
}